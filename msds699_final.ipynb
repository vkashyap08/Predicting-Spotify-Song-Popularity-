{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "labeled-separation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:28:20.430386Z",
     "start_time": "2021-03-07T04:28:19.803213Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-share",
   "metadata": {},
   "source": [
    "## Loading The Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-treatment",
   "metadata": {},
   "source": [
    "- Dropping column \"id\" as it can be used to explicitly identify the sample. \n",
    "- Dropping column \"name\" as there are several non-ascii characters, plus this column would not really contribute to predicting popularity since it is simply the name of the song. \n",
    "- Dropping column \"release-date\" because column \"year\" is a cleaned up version of the same data.\n",
    "- Choosing data released in 2018 onwards to reduce the massive size of the dataset. I'm also more interested in recent music. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "crucial-concord",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:28:21.018841Z",
     "start_time": "2021-03-07T04:28:20.432881Z"
    }
   },
   "outputs": [],
   "source": [
    "original = pd.read_csv(\"/Users/vaishnavikashyap/classes/msds699/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "scenic-honolulu",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:28:21.045911Z",
     "start_time": "2021-03-07T04:28:21.021511Z"
    }
   },
   "outputs": [],
   "source": [
    "data = original.drop([\"id\", \"release_date\", 'name'], axis=1)\n",
    "data = data[(data['year']>2018)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-saying",
   "metadata": {},
   "source": [
    "## Dividing feature columns into categorical and numerical for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aging-genealogy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:28:21.062938Z",
     "start_time": "2021-03-07T04:28:21.057713Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_columns = ['artists']\n",
    "numerical_columns = ['acousticness', 'danceability', 'duration_ms', 'energy', \n",
    "                     'explicit', 'instrumentalness', 'key',\n",
    "                     'liveness', 'loudness', 'mode', 'speechiness', 'tempo', 'valence', 'year']\n",
    "\n",
    "X = data[categorical_columns + numerical_columns]\n",
    "y = data[\"popularity\"]\n",
    "y = y.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-developer",
   "metadata": {},
   "source": [
    "## 80/20 Train-Test split and 80/20 Train-Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cross-genius",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:28:21.073012Z",
     "start_time": "2021-03-07T04:28:21.064822Z"
    }
   },
   "outputs": [],
   "source": [
    "# splitting into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2)\n",
    "\n",
    "# splitting train into train and validation \n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size= 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-vinyl",
   "metadata": {},
   "source": [
    "## Creating Initial Pipe with Dummy Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-quarterly",
   "metadata": {},
   "source": [
    "I ran the one categorical column \"artists\" through OneHotEncoder so that a float value could be assigned to each artist name. I ran the numerical columns through SimpleImputer so that any missing values would be fit with the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "communist-daily",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:28:21.077912Z",
     "start_time": "2021-03-07T04:28:21.074777Z"
    }
   },
   "outputs": [],
   "source": [
    "class DummyEstimator(BaseEstimator):\n",
    "    \"Pass through class, methods are present but do nothing.\"\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "raised-hampshire",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:28:21.085570Z",
     "start_time": "2021-03-07T04:28:21.082286Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('preprocess', ColumnTransformer([('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "                                      ('num', SimpleImputer(strategy='mean'), numerical_columns)])),\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('clf', DummyEstimator())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-robert",
   "metadata": {},
   "source": [
    "## Randomized Search CV to Hypertune Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-increase",
   "metadata": {},
   "source": [
    "I chose to focus on Random Forest Regressor, Decision Tree Regressor, and a basic Linear Regression since the goal is to develop a model that predicts the popularity (measured on a scale of 1 to 100) of a track on Spotify. Through this randomized cross validation search, I'll hopefully be able to find the best model that will give me the lowest evaluation metrics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "complicated-princess",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:29:09.160704Z",
     "start_time": "2021-03-07T04:28:21.089114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:   35.6s finished\n"
     ]
    }
   ],
   "source": [
    "search_space = [{'clf': [RandomForestRegressor()], \n",
    "                 'clf__n_estimators': range(1,100),\n",
    "                 'clf__max_features': ['sqrt', 'log2', 'auto'],\n",
    "                 'clf__min_samples_leaf': range(1,10),\n",
    "                 'clf__min_samples_split': range(1,10)},\n",
    "        \n",
    "                {'clf': [DecisionTreeRegressor()],\n",
    "                 'clf__max_depth': [None, 5, 6, 7, 8, 9, 10, 11],\n",
    "                 'clf__max_features': [None, 'sqrt', 'auto', 'log2', 0.3, 0.5, 0.7],\n",
    "                 'clf__min_samples_leaf': [1, 0.3, 0.5],\n",
    "                 'clf__criterion': [\"gini\", \"entropy\"]},\n",
    "                \n",
    "                {'clf': [LinearRegression()]}]\n",
    "\n",
    "# Random Search\n",
    "random_search = RandomizedSearchCV(estimator=pipe, \n",
    "                                    param_distributions=search_space, \n",
    "                                    n_iter=50,\n",
    "                                    cv=3, \n",
    "                                    n_jobs=-1,\n",
    "                                    verbose=1)\n",
    "#  Fit grid search\n",
    "best_model = random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "humanitarian-hometown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:29:09.174236Z",
     "start_time": "2021-03-07T04:29:09.163457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=49)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.best_estimator_.get_params()['clf']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-partition",
   "metadata": {},
   "source": [
    "Randomized Search CV said that the only hyperparam I should hypertune is:\n",
    "- n_estimators=49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-division",
   "metadata": {},
   "source": [
    "### Random Forest Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "driving-falls",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:29:09.184004Z",
     "start_time": "2021-03-07T04:29:09.178466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'mse',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-capability",
   "metadata": {},
   "source": [
    "## Final Tuned Hyperparameters List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "painful-intensity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:30:05.499485Z",
     "start_time": "2021-03-07T04:30:05.494272Z"
    }
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    " 'bootstrap': True,\n",
    " 'ccp_alpha': 0.0,\n",
    " 'criterion': 'mse',\n",
    " 'max_depth': None,\n",
    " 'max_features': 'auto',\n",
    " 'max_leaf_nodes': None,\n",
    " 'max_samples': None,\n",
    " 'min_impurity_decrease': 0.0,\n",
    " 'min_impurity_split': None,\n",
    " 'min_samples_leaf': 1,\n",
    " 'min_samples_split': 2,\n",
    " 'min_weight_fraction_leaf': 0.0,\n",
    " 'n_estimators': 49,\n",
    " 'n_jobs': None,\n",
    " 'oob_score': False,\n",
    " 'random_state': None,\n",
    " 'verbose': 0,\n",
    " 'warm_start': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-passport",
   "metadata": {},
   "source": [
    "## Fitting Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "removed-trace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:30:20.878394Z",
     "start_time": "2021-03-07T04:30:08.383051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocess',\n",
       "                 ColumnTransformer(transformers=[('cat',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['artists']),\n",
       "                                                 ('num', SimpleImputer(),\n",
       "                                                  ['acousticness',\n",
       "                                                   'danceability',\n",
       "                                                   'duration_ms', 'energy',\n",
       "                                                   'explicit',\n",
       "                                                   'instrumentalness', 'key',\n",
       "                                                   'liveness', 'loudness',\n",
       "                                                   'mode', 'speechiness',\n",
       "                                                   'tempo', 'valence',\n",
       "                                                   'year'])])),\n",
       "                ('scaler', StandardScaler(with_mean=False)),\n",
       "                ('classifier', RandomForestRegressor(n_estimators=49))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('preprocess', ColumnTransformer([('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "                                      ('num', SimpleImputer(strategy='mean'), numerical_columns)])),\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('classifier', RandomForestRegressor(**hyperparameters))\n",
    "])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-eleven",
   "metadata": {},
   "source": [
    "## Evaluation Metrics for Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-alignment",
   "metadata": {},
   "source": [
    "- RF train/test accuracy\n",
    "- MSE (Mean Squared Error)\n",
    "- MAE (Mean Absolute Error)\n",
    "\n",
    "A combination of these 3 metrics will clearly tell me whether my model accurately predicted the popularity of each song. MAE measures the absolute average distance between the real data and the predicted data but fails to punish large errors in prediction. MSE measures the squared average distance between the real data and the predicted data and takes into account larger errors. Although it's hard to compare the two, I would ideally have low numbers for both.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "passing-edition",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:30:24.255369Z",
     "start_time": "2021-03-07T04:30:24.111596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF train accuracy: 0.944\n",
      "RF validation accuracy: 0.625\n",
      "RF test accuracy: 0.631\n"
     ]
    }
   ],
   "source": [
    "print(\"RF train accuracy: %0.3f\" % pipe.score(X_train, y_train))\n",
    "print(\"RF validation accuracy: %0.3f\" % pipe.score(X_validation, y_validation))\n",
    "print(\"RF test accuracy: %0.3f\" % pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "physical-actress",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:30:29.102750Z",
     "start_time": "2021-03-07T04:30:29.058091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 354.51\n",
      "Mean absolute error: 10.70\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipe.predict(X_validation)\n",
    "mse = mean_squared_error(y_validation, y_pred)\n",
    "mae = mean_absolute_error(y_validation, y_pred)\n",
    "print(f\"Mean squared error: {mse:,.2f}\")\n",
    "print(f\"Mean absolute error: {mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "novel-patient",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:30:31.251897Z",
     "start_time": "2021-03-07T04:30:31.204056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 333.20\n",
      "Mean absolute error: 10.44\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipe.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean squared error: {mse:,.2f}\")\n",
    "print(f\"Mean absolute error: {mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-collar",
   "metadata": {},
   "source": [
    "The train accuracy is high and leads me to believe the data has been overfit on the training dataset since the test accuracy is much lower than I would have predicted. Below is a dataframe I created which puts the actual y_test values and the predicted y_pred values side by side to compare. We can see that for example rows 1688, 1698, 1690 are pretty similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "circular-affect",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T05:14:19.829256Z",
     "start_time": "2021-03-07T05:14:19.815814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>37.061224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.306122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>5.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>21.040816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>12.632653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>0</td>\n",
       "      <td>0.877551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1689</th>\n",
       "      <td>5</td>\n",
       "      <td>4.734694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>25</td>\n",
       "      <td>23.224490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>25</td>\n",
       "      <td>19.081633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>31</td>\n",
       "      <td>27.551020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1693 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      y_test     y_pred\n",
       "0         71  37.061224\n",
       "1          1   4.306122\n",
       "2         26   5.714286\n",
       "3          0  21.040816\n",
       "4         15  12.632653\n",
       "...      ...        ...\n",
       "1688       0   0.877551\n",
       "1689       5   4.734694\n",
       "1690      25  23.224490\n",
       "1691      25  19.081633\n",
       "1692      31  27.551020\n",
       "\n",
       "[1693 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'y_test':y_test, 'y_pred':y_pred})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-swiss",
   "metadata": {},
   "source": [
    "## Quick Comparison with default Random Forest, default Decision Tree, and default Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "other-torture",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:31:03.102544Z",
     "start_time": "2021-03-07T04:30:37.685757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 330.83\n",
      "Mean absolute error: 10.25\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('preprocess', ColumnTransformer([('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "                                      ('num', SimpleImputer(strategy='mean'), numerical_columns)])),\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('rf', RandomForestRegressor()),])\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean squared error: {mse:,.2f}\")\n",
    "print(f\"Mean absolute error: {mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "narrative-buying",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:31:12.340381Z",
     "start_time": "2021-03-07T04:31:11.908398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 552.16\n",
      "Mean absolute error: 10.87\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('preprocess', ColumnTransformer([('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "                                      ('num', SimpleImputer(strategy='mean'), numerical_columns)])),\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('dt', DecisionTreeRegressor())])\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean squared error: {mse:,.2f}\")\n",
    "print(f\"Mean absolute error: {mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "detailed-boards",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:31:14.933993Z",
     "start_time": "2021-03-07T04:31:14.874488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 400.86\n",
      "Mean absolute error: 12.09\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('preprocess', ColumnTransformer([('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "                                      ('num', SimpleImputer(strategy='mean'), numerical_columns)])),\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('lr', LinearRegression())])\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean squared error: {mse:,.2f}\")\n",
    "print(f\"Mean absolute error: {mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-toner",
   "metadata": {},
   "source": [
    "### Chart Detailing Evaluation Metrics of all Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "distributed-treasurer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:36:22.533523Z",
     "start_time": "2021-03-07T04:36:22.522328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tuned Random Forest</td>\n",
       "      <td>10.44</td>\n",
       "      <td>333.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Default Random Forest</td>\n",
       "      <td>10.25</td>\n",
       "      <td>330.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Default Decision Tree</td>\n",
       "      <td>10.87</td>\n",
       "      <td>552.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Default Linear Regression</td>\n",
       "      <td>12.09</td>\n",
       "      <td>400.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model    MAE     MSE\n",
       "0        Tuned Random Forest  10.44  333.20\n",
       "1      Default Random Forest  10.25  330.83\n",
       "2      Default Decision Tree  10.87  552.16\n",
       "3  Default Linear Regression  12.09  400.86"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_df = pd.DataFrame(np.array([['Tuned Random Forest', '10.44', '333.20'], \n",
    "                                ['Default Random Forest', '10.25', '330.83'], \n",
    "                                ['Default Decision Tree', '10.87', '552.16'],\n",
    "                                ['Default Linear Regression', '12.09', '400.86']]),\n",
    "                   columns=['Model', 'MAE', 'MSE'])\n",
    "mae_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-intelligence",
   "metadata": {},
   "source": [
    "We can see from the above that the default Random Forest, which was only different from the tuned Random Forest in that it had the default of n_estimators = 100, actually performed the best, followed by tuned Random Forest, default Linear Regression as third, and default Decision Tree Regressor in last place. Overall, the MAEs were smaller than expected which is great. The fact that default RF performed the best shows that a Random Forest model is very versatile for different kinds of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-thermal",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-morrison",
   "metadata": {},
   "source": [
    "In summary, my evaluation metrics were much higher than I hoped. One thing I would change would be to do a more robust Randomized Search or Grid Search that would include more variety in models. Additionally, I believe further EDA was needed in order to really lock down which features were truly important to the prediction of popularity, rather than using almost all the columns in the dataset. In a future iteration of this project, I would use the \"permutation importance\" sklearn package first to choose the best features for my model. It would also be interesting to see how feature importance changed over the years. Using features such as \"danceability\", \"energy\", etc to predict a song's popularity could also be crucial to creating a more personalized user experience for music streaming giants like Spotify and Pandora. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-basketball",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "https://www.kaggle.com/yamaerenay/spotify-dataset-19212020-160k-tracks\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples//inspection/plot_permutation_importance.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
